% Maximum likelihood estimation for the general linear model
% _
% Author: Joram Soch, BCCN Berlin
% E-Mail: joram.soch@bccn-berlin.de
% Edited: 07/05/2019, XX:XX


\setcounter{equation}{0}
\input{Index}


\paragraph{Theorem:}

Given a general linear model with matrix-normally distributed errors

\begin{equation} \label{eq:GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; ,
\end{equation}

maximum likelihood estimates for the unknown parameters $B$ and $\Sigma$ are given by

\vspace{-0.5em}
\begin{equation} \label{eq:GLM-MLE}
\begin{split}
\hat{B} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \\
\hat{\Sigma} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; . \\
\end{split}
\end{equation}


\paragraph{Proof:} In (\ref{eq:GLM}), $Y$ is an $n \times v$ matrix of measurements ($n$ observiations, $v$ dependent variables), $X$ is an $n \times p$ design matrix ($n$ observiations, $p$ independent variables) and $V$ is an $n \times n$ covariance matrix across observations. This multivariate GLM implies the following likelihood function

\vspace{-0.5em}
\begin{equation} \label{eq:GLM-LF}
\begin{split}
p(Y|B,\Sigma) &= \mathcal{MN}(Y; XB, V, \Sigma) \\
&= \sqrt{\frac{1}{(2\pi)^{nv} |\Sigma|^n |V|^v}} \cdot \exp\left[ -\frac{1}{2} \, \mathrm{tr}\left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right)  \right] \\
\end{split}
\end{equation}

and the log-likelihood function

\vspace{-0.5em}
\begin{equation} \label{eq:GLM-LL1}
\begin{split}
\mathrm{LL}(B,\Sigma) = &\log p(Y|B,\Sigma) \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\Sigma| - \frac{v}{2} \log |V| \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \; .\\
\end{split}
\end{equation}

Substituting $V^{-1}$ by the precision matrix $P$ to ease notation, we have:

\vspace{-0.5em}
\begin{equation} \label{eq:GLM-LL2}
\begin{split}
\mathrm{LL}(B,\Sigma) = &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log(|\Sigma|) - \frac{v}{2} \log(|V|) \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \; .\\
\end{split}
\end{equation}

The derivative of the log-likelihood function (\ref{eq:GLM-LL2}) with respect to $B$ is

\vspace{-0.5em}
\begin{equation} \label{eq:dLL-dB}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(B,\Sigma)}{\mathrm{d}B} &= \frac{\mathrm{d}}{\mathrm{d}B} \left( - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \right) \\
&= \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ -2 \Sigma^{-1} Y^\mathrm{T} P X B \right] \right) + \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} B^\mathrm{T} X^\mathrm{T} P X B \right] \right) \\
&= - \frac{1}{2} \left( -2 X^\mathrm{T} P Y \Sigma^{-1} \right) - \frac{1}{2} \left( X^\mathrm{T} P X B \Sigma^{-1} + (X^\mathrm{T} P X)^\mathrm{T} B (\Sigma^{-1})^\mathrm{T} \right) \\
&= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X B \Sigma^{-1} \\
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $B$:

\vspace{-0.5em}
\begin{equation} \label{eq:B-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\Sigma)}{\mathrm{d}B} &= 0 \\
0 &= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X \hat{B} \Sigma^{-1} \\
0 &= X^\mathrm{T} P Y - X^\mathrm{T} P X \hat{B} \\
X^\mathrm{T} P X \hat{B} &= X^\mathrm{T} P Y \\
\hat{B} &= \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P Y \\
\end{split}
\end{equation}

\vspace{-2.25em}
\hspace\fill $\square$

\vspace{0.75em}
The derivative of the log-likelihood function (\ref{eq:GLM-LL1}) with respect to $B$ is

\vspace{-0.5em}
\begin{equation} \label{eq:dLL-dS}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(B,\Sigma)}{\mathrm{d}\Sigma} &= \frac{\mathrm{d}}{\mathrm{d}\Sigma} \left( - \frac{n}{2} \log |\Sigma| - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \right) \\
&= - \frac{n}{2} \left( \Sigma^{-1} \right)^\mathrm{T} + \frac{1}{2} \left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \Sigma^{-1} \right)^\mathrm{T} \\
&= - \frac{n}{2} \, \Sigma^{-1} + \frac{1}{2} \, \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \Sigma^{-1} \\
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\Sigma$:

\vspace{-0.5em}
\begin{equation} \label{eq:S-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(B,\hat{\Sigma})}{\mathrm{d}\Sigma} &= 0 \\
0 &= - \frac{n}{2} \, \hat{\Sigma}^{-1} + \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \hat{\Sigma}^{-1} \\
\frac{n}{2} \, \hat{\Sigma}^{-1} &= \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma}^{-1} &= \frac{1}{n} \, \hat{\Sigma}^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \hat{\Sigma}^{-1} \\
I_v &= \frac{1}{n} \, (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma} &= \frac{1}{n} \, (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \\
\end{split}
\end{equation}

\vspace{-2.25em}
\hspace\fill $\square$

\vspace{0.75em}
Together, (\ref{eq:B-MLE}) and (\ref{eq:S-MLE}) constitute the MLE for the GLM. \hspace\fill $\blacksquare$


\paragraph{Dependencies:}
\begin{itemize}
\item probability density function of the matrix-normal distribution
\item maximum likelihood estimation for a generative model
\end{itemize}


\paragraph{Source:}
\begin{itemize}
\item to be added
\end{itemize}


\input{Meta}
\Meta{A007}{glm-mle}{JoramSoch}{2019-05-07}