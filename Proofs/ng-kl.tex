% Kullback-Leibler divergence of the normal-gamma distribution
% _
% Author: Joram Soch, BCCN Berlin
% E-Mail: joram.soch@bccn-berlin.de
% Edited: 07/05/2019, 14:00


\setcounter{equation}{0}
\input{Index}


\paragraph{Theorem:}

Let $x \in \mathbb{R}^k$ be a random vector and $y > 0$ be a random variable. Assume two normal-gamma distributions $P$ and $Q$ specifying the joint distribution of $x$ and $y$ as

\vspace{-0.5em}
\begin{equation} \label{eq:NGs}
\begin{split}
P: \; (x,y) &\sim \mathrm{NG}(\mu_1, \Lambda_1^{-1}, a_1, b_1) \\
Q: \; (x,y) &\sim \mathrm{NG}(\mu_2, \Lambda_2^{-1}, a_2, b_2) \; . \\
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence of $P$ from $Q$ is given by

\vspace{-0.5em}
\begin{equation} \label{eq:NG-KL}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \frac{a_1}{b_1} \left[ (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) \right] + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \\
&+ a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; .
\end{split}
\end{equation}


\paragraph{Proof:} The probabibility density function of the normal-gamma (NG) distribution is

\begin{equation} \label{eq:NG-pdf}
p(x,y) = p(x|y) \cdot p(y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b)
\end{equation}

where $\mathcal{N}(x; \mu, \Sigma)$ is a multivariate normal density with mean $\mu$ and covariance $\Sigma$ (hence, precision $\Lambda$) and $\mathrm{Gam}(y; a, b)$ is a univariate gamma density with shape $a$ and rate $b$. The Kullback-Leibler (KL) divergence of the multivariate normal distribution is

\begin{equation} \label{eq:mvn-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^T \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - k \right]
\end{equation}

and the Kullback-Leibler divergence of the univariate gamma distribution is

\begin{equation} \label{eq:gam-KL}
\mathrm{KL}[P\,||\,Q] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\psi(x)$ is the digamma function.

\vspace{1em}
The KL divergence for a continuous random variable is given by 

\begin{equation} \label{eq:KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{Z} p(z) \, \ln \frac{p(z)}{q(z)} \, \mathrm{d}z
\end{equation}

which, applied to the normal-gamma distribution over $x$ and $y$, yields

\begin{equation} \label{eq:NG-KL0}
\mathrm{KL}[P\,||\,Q] = \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x,y) \, \ln \frac{p(x,y)}{q(x,y)} \, \mathrm{d}x \, \mathrm{d}y \; .
\end{equation}

Using the law of conditional probability, this can be evaluated as follows:

\vspace{-0.5em}
\begin{equation} \label{eq:NG-KL1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y) \, p(y) \, \ln \frac{p(x|y) \, p(y)}{q(x|y) \, q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y)\, p(y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y)\, p(y) \, \ln \frac{p(y)}{q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} p(y) \int_{\mathbb{R}^k} p(x|y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} p(y) \, \ln \frac{p(y)}{q(y)} \int_{\mathbb{R}^k} p(x|y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} + \mathrm{KL}[p(y)\,||\,q(y)] \; .
\end{split}
\end{equation}

In other words, the KL divergence between two normal-gamma distributions over $x$ and $y$ is equal to the sum of a multivariate normal KL divergence regarding $x$ conditional on $y$, expected over $y$, and a univariate gamma KL divergence regarding $y$.

From equations (\ref{eq:NG-pdf}) and (\ref{eq:mvn-KL}), the first term becomes

\vspace{-0.5em}
\begin{equation}
\begin{split}
&\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} \\
&= \left\langle \frac{1}{2} \left[ (\mu_2 - \mu_1)^T (y \Lambda_2) (\mu_2 - \mu_1) + \mathrm{tr}\left( (y \Lambda_2) (y \Lambda_1)^{-1} \right) - \ln \frac{|(y \Lambda_1)^{-1}|}{|(y \Lambda_2)^{-1}|} - k \right] \right\rangle_{p(y)} \\
&= \left\langle \frac{y}{2} (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \right\rangle_{p(y)} \\
\end{split}
\end{equation}

and using the relation $y \sim \mathrm{Gam}(a,b) \Rightarrow \left\langle y \right\rangle = a/b$, we have

\vspace{-0.5em}
\begin{equation} \label{eq:exp-mvn-KL}
\begin{split}
\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} = \frac{1}{2} \frac{a_1}{b_1} (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \; .
\end{split}
\end{equation}

By plugging (\ref{eq:exp-mvn-KL}) and (\ref{eq:gam-KL}) into (\ref{eq:NG-KL1}), one arrives at the KL divergence given by (\ref{eq:NG-KL}). \hspace\fill $\blacksquare$


\paragraph{Dependencies:}
\begin{itemize}
\item probability density function of the normal-gamma distribution
\item Kullback-Leibler divergence of the multivariate normal distribution
\item Kullback-Leibler divergence of the univariate gamma distribution
\item Kullback-Leibler divergence for a continuous random variable
\item law of conditional probability, also called "product rule of probability"
\item expected value of a gamma random variable
\end{itemize}


\paragraph{Source:}
\begin{itemize}
\item Soch \& Allefeld (2016): "Kullback-Leibler Divergence for the Normal-Gamma Distribution". \textit{arXiv math.ST}, 1611.01437; URL: \url{https://arxiv.org/abs/1611.01437}.
\end{itemize}


\input{Meta}
\Meta{A006}{ng-kl}{JoramSoch}{2019-05-07}